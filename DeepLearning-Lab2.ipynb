{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1660793,
     "status": "ok",
     "timestamp": 1747930322321,
     "user": {
      "displayName": "Yaojia Wang",
      "userId": "02583595600482354486"
     },
     "user_tz": -480
    },
    "id": "RmcMo9k35BdZ",
    "outputId": "0051a8a1-86e4-4d54-cec3-2624cbfbf38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0,loss:3.831463098526001\n",
      "iter:1000,loss:0.5947991013526917\n",
      "iter:2000,loss:0.10980270802974701\n",
      "iter:3000,loss:0.016202563419938087\n",
      "iter:4000,loss:0.14844641089439392\n",
      "iter:5000,loss:0.05808998644351959\n",
      "iter:6000,loss:0.030240148305892944\n",
      "iter:7000,loss:0.06617588549852371\n",
      "iter:8000,loss:0.010835248976945877\n",
      "iter:9000,loss:0.2623428702354431\n",
      "iter:10000,loss:0.009947315789759159\n",
      "iter:11000,loss:0.05500946193933487\n",
      "iter:12000,loss:0.02327805757522583\n",
      "iter:13000,loss:0.02404194325208664\n",
      "iter:14000,loss:0.010703436098992825\n",
      "iter:15000,loss:0.0014321808703243732\n",
      "iter:16000,loss:0.0544450618326664\n",
      "iter:17000,loss:0.00264459615573287\n",
      "iter:18000,loss:0.026783445850014687\n",
      "iter:19000,loss:0.0451720654964447\n",
      "iter:20000,loss:0.009238595142960548\n",
      "iter:21000,loss:0.001791806542314589\n",
      "iter:22000,loss:0.12152861058712006\n",
      "iter:23000,loss:0.011750077828764915\n",
      "iter:24000,loss:0.01743306964635849\n",
      "iter:25000,loss:0.0005077047972008586\n",
      "iter:26000,loss:0.0022241934202611446\n",
      "iter:27000,loss:0.004894816316664219\n",
      "iter:28000,loss:0.09610830247402191\n",
      "iter:29000,loss:0.030718713998794556\n",
      "iter:30000,loss:0.037947043776512146\n",
      "iter:31000,loss:0.001599698094651103\n",
      "iter:32000,loss:0.001714728306978941\n",
      "iter:33000,loss:0.02327297255396843\n",
      "iter:34000,loss:0.005216340534389019\n",
      "iter:35000,loss:0.00530783087015152\n",
      "iter:36000,loss:0.1576233208179474\n",
      "iter:37000,loss:0.0015017057303339243\n",
      "iter:38000,loss:0.0004946865374222398\n",
      "iter:39000,loss:0.0029961341060698032\n",
      "iter:40000,loss:0.003815164091065526\n",
      "iter:41000,loss:0.010332724079489708\n",
      "iter:42000,loss:0.2924327850341797\n",
      "iter:43000,loss:0.16284900903701782\n",
      "iter:44000,loss:0.0024645242374390364\n",
      "iter:45000,loss:0.0067975157871842384\n",
      "iter:46000,loss:0.0006224013632163405\n",
      "iter:47000,loss:0.0021357822697609663\n",
      "iter:48000,loss:0.020769935101270676\n",
      "iter:49000,loss:0.0004063103988301009\n",
      "iter:50000,loss:0.03779495880007744\n",
      "iter:51000,loss:0.01987118273973465\n",
      "iter:52000,loss:0.044960565865039825\n",
      "iter:53000,loss:0.0009121540933847427\n",
      "iter:54000,loss:0.005164847709238529\n",
      "iter:55000,loss:0.007138518616557121\n",
      "iter:56000,loss:0.0013856859877705574\n",
      "iter:57000,loss:0.012899082154035568\n",
      "iter:58000,loss:0.0023701859172433615\n",
      "iter:59000,loss:0.20199136435985565\n",
      "iter:60000,loss:0.001255244598723948\n",
      "iter:61000,loss:0.009115601889789104\n",
      "iter:62000,loss:0.0009693136671558022\n",
      "iter:63000,loss:0.0054940893314778805\n",
      "iter:64000,loss:0.0022904600482434034\n",
      "iter:65000,loss:0.022251039743423462\n",
      "iter:66000,loss:0.05482611805200577\n",
      "iter:67000,loss:0.0002517598622944206\n",
      "iter:68000,loss:0.04210254177451134\n",
      "iter:69000,loss:0.004964441526681185\n",
      "iter:70000,loss:0.004172575660049915\n",
      "iter:71000,loss:0.0011823689565062523\n",
      "iter:72000,loss:0.17146851122379303\n",
      "iter:73000,loss:0.12594574689865112\n",
      "iter:74000,loss:0.003223668085411191\n",
      "iter:75000,loss:0.009310286492109299\n",
      "iter:76000,loss:0.00882716104388237\n",
      "iter:77000,loss:0.017682816833257675\n",
      "iter:78000,loss:0.0004291109798941761\n",
      "iter:79000,loss:0.09818268567323685\n",
      "iter:80000,loss:0.0004182968405075371\n",
      "iter:81000,loss:0.014358768239617348\n",
      "iter:82000,loss:0.0015655325260013342\n",
      "iter:83000,loss:0.008792626671493053\n",
      "iter:84000,loss:0.01719079352915287\n",
      "iter:85000,loss:0.3762493133544922\n",
      "iter:86000,loss:0.008280569687485695\n",
      "iter:87000,loss:0.0010496918112039566\n",
      "iter:88000,loss:0.005363179370760918\n",
      "iter:89000,loss:0.007199297659099102\n",
      "iter:90000,loss:0.00012181499187136069\n",
      "iter:91000,loss:0.00048350804718211293\n",
      "iter:92000,loss:0.015133863314986229\n",
      "iter:93000,loss:0.0012012980878353119\n",
      "iter:94000,loss:0.2113655060529709\n",
      "iter:95000,loss:0.1803375780582428\n",
      "iter:96000,loss:0.0009924436453729868\n",
      "iter:97000,loss:0.01939208246767521\n",
      "iter:98000,loss:0.0038450113497674465\n",
      "iter:99000,loss:0.0010074801975861192\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import MNIST\n",
    "from clip import CLIP\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "def main():\n",
    "    DEVICE='cuda' if torch.cuda.is_available() else 'cpu'   # 设备\n",
    "\n",
    "    dataset=MNIST() # 数据集\n",
    "\n",
    "    model=CLIP().to(DEVICE) # 模型\n",
    "\n",
    "    optimzer=torch.optim.Adam(model.parameters(),lr=1e-3)   # 优化器\n",
    "\n",
    "    '''\n",
    "        训练模型\n",
    "    '''\n",
    "    ITER_BATCH_COUNT=100000    # 迭代次数\n",
    "    BATCH_SIZE=64   # 从batch内选出10个不一样的数字\n",
    "    TARGET_COUNT=10 # 共10种数字\n",
    "\n",
    "    # 修改worker数量为0可以避免多进程问题\n",
    "    dataloader=DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)    # 数据加载器\n",
    "\n",
    "    for i in range(ITER_BATCH_COUNT):\n",
    "        while True:\n",
    "            imgs,labels=next(iter(dataloader))\n",
    "            if torch.unique(labels).shape[0]<TARGET_COUNT:  # 未覆盖10种数字\n",
    "                continue\n",
    "            # 挑选出10个数字\n",
    "            target=set()\n",
    "            indexes=[]\n",
    "            for j in range(BATCH_SIZE):\n",
    "                if labels[j].item() in target:\n",
    "                    continue\n",
    "                target.add(labels[j].item())\n",
    "                indexes.append(j)\n",
    "                if len(target)==TARGET_COUNT:\n",
    "                    break\n",
    "            imgs=imgs[indexes]\n",
    "            labels=labels[indexes]\n",
    "            break\n",
    "\n",
    "        ### ====== TODO: TASK2: 完成模型损失函数计算的代码（BEGIN）\n",
    "\n",
    "        # 通过模型获取相似度矩阵\n",
    "        logits = model(imgs.to(DEVICE), labels.to(DEVICE))\n",
    "\n",
    "        # 对比学习中，第i个图像应该与第i个文本匹配\n",
    "        labels_one_hot = torch.arange(TARGET_COUNT, device=DEVICE)\n",
    "\n",
    "        # 计算双向对比损失（图像到文本 + 文本到图像）\n",
    "        loss = (F.cross_entropy(logits, labels_one_hot) +\n",
    "                F.cross_entropy(logits.t(), labels_one_hot)) / 2\n",
    "\n",
    "        ### ====== TODO: TASK 2: 完成模型损失函数计算的代码（END）\n",
    "\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        if i%1000==0:\n",
    "            print('iter:{},loss:{}'.format(i,loss))\n",
    "            torch.save(model.state_dict(),'.model.pth')\n",
    "            os.replace('.model.pth','model.pth')\n",
    "\n",
    "# 重要！添加程序入口保护\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/hx0fdggx1yd005l9y1x7hszw0000gn/T/ipykernel_34887/3011201794.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset\u001b[38;5;241m=\u001b[39mMNIST() \u001b[38;5;66;03m# 数据集\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m=\u001b[39mCLIP()\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;66;03m# 模型\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()    \u001b[38;5;66;03m# 预测模式\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m1、对图片分类\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1787\u001b[0m )\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "'''\n",
    "CLIP能力演示\n",
    "\n",
    "1、对图片做分类\n",
    "2、对图片求相图片\n",
    "\n",
    "'''\n",
    "\n",
    "from dataset import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from clip import CLIP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'   # 设备\n",
    "\n",
    "dataset=MNIST() # 数据集\n",
    "\n",
    "model=CLIP().to(DEVICE) # 模型\n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "\n",
    "model.eval()    # 预测模式\n",
    "\n",
    "'''\n",
    "1、对图片分类\n",
    "'''\n",
    "image,label=dataset[0]\n",
    "print('正确分类:',label)\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### TODO: TASK 3: 完成CLIP模型进行预测的代码 (BEGIN)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 为每个数字(0-9)生成文本嵌入\n",
    "    all_text_embeddings = model.text_enc(torch.arange(10).to(DEVICE))\n",
    "    # 获取图像嵌入\n",
    "    image_embedding = model.img_enc(image.unsqueeze(0).to(DEVICE))\n",
    "\n",
    "    # 规范化嵌入\n",
    "    image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n",
    "    all_text_embeddings = all_text_embeddings / all_text_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # 计算相似度\n",
    "    similarity = torch.mm(image_embedding, all_text_embeddings.t())\n",
    "    print(similarity)\n",
    "\n",
    "    # 选择相似度最高的作为预测结果\n",
    "    predicted_label = similarity.argmax(dim=1).item()\n",
    "\n",
    "### TODO: TASK 3: 完成CLIP模型进行预测的代码 (END)\n",
    "\n",
    "print('CLIP分类:', predicted_label)\n",
    "\n",
    "'''\n",
    "2、图像相似度\n",
    "'''\n",
    "other_images=[]\n",
    "other_labels=[]\n",
    "for i in range(1,101):\n",
    "    other_image,other_label=dataset[i]\n",
    "    other_images.append(other_image)\n",
    "    other_labels.append(other_label)\n",
    "\n",
    "### TODO: TASK 4: 使用CLIP的image encoder，从other_images里检索和image最相似的5张图像 (BEGIN)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 将其他图像转换为张量\n",
    "    other_images_tensor = torch.stack(other_images).to(DEVICE)\n",
    "\n",
    "    # 使用图像编码器获取图像嵌入\n",
    "    query_embedding = model.img_enc(image.unsqueeze(0).to(DEVICE))\n",
    "    other_embeddings = model.img_enc(other_images_tensor)\n",
    "\n",
    "    # 规范化嵌入\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    other_embeddings = other_embeddings / other_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # 计算相似度\n",
    "    similarities = torch.mm(query_embedding, other_embeddings.t())\n",
    "\n",
    "    # 获取相似度最高的5个索引\n",
    "    indexs = similarities[0].topk(5).indices.cpu().numpy().tolist()\n",
    "\n",
    "### TODO: TASK 4: 使用CLIP的image encoder，从other_images里检索和image最相似的5张图像 (END)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i,img_idx in enumerate(indexs):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(other_images[img_idx].permute(1,2,0))\n",
    "    plt.title(other_labels[img_idx])\n",
    "    plt.axis('off')\n",
    "plt.savefig(f\"output/similarity{label}.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# 在文件末尾添加\n",
    "\n",
    "'''\n",
    "3、在整个MNIST数据集上评估CLIP模型性能\n",
    "'''\n",
    "print(\"\\n在整个MNIST数据集上评估CLIP模型性能:\")\n",
    "\n",
    "# 使用tqdm创建进度条(如果没有安装可以使用pip install tqdm安装)\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 为每个数字(0-9)生成文本嵌入(只需计算一次)\n",
    "    all_text_embeddings = model.text_enc(torch.arange(10).to(DEVICE))\n",
    "    all_text_embeddings = all_text_embeddings / all_text_embeddings.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # 遍历数据集中的所有样本\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        image, label = dataset[i]\n",
    "        \n",
    "        # 获取图像嵌入\n",
    "        image_embedding = model.img_enc(image.unsqueeze(0).to(DEVICE))\n",
    "        image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        # 计算相似度\n",
    "        similarity = torch.mm(image_embedding, all_text_embeddings.t())\n",
    "        \n",
    "        # 选择相似度最高的作为预测结果\n",
    "        predicted_label = similarity.argmax(dim=1).item()\n",
    "        \n",
    "        # 统计正确预测的数量\n",
    "        total += 1\n",
    "        if predicted_label == label:\n",
    "            correct += 1\n",
    "            class_correct[label] += 1\n",
    "        class_total[label] += 1\n",
    "\n",
    "# 计算并打印总体准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'模型在MNIST数据集上的总体准确率: {accuracy:.2f}%')\n",
    "print(f'正确预测: {correct}/{total}')\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "print(\"\\n各数字类别的准确率:\")\n",
    "for i in range(10):\n",
    "    class_acc = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'数字 {i}: {class_acc:.2f}% ({class_correct[i]}/{class_total[i]})')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMANoy6duUDAPu2d30+EQBm",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
